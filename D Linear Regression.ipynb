{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 222](https://github.com/GonzagaCPSC222) Intro to Data Science\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Linear Regression\n",
    "What are our learning objectives for this lesson?\n",
    "* Calculate a least squares linear regression line\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s)\n",
    "In ClassificationFun:\n",
    "* Download the AmesHousing.csv file from the DAs repository on Github\n",
    "* In a new notebook called LinearRegression.ipynb\n",
    "    * Read in AmesHousing.csv into a dataframe\n",
    "    * Create a scatter plot of SalePrice on the y-axis and a numeric attribute on the x-axis you think is predictive of how much a house sells for (attribute descriptions available [here](https://jse.amstat.org/v19n3/decock/DataDocumentation.txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today\n",
    "* Announcements\n",
    "    * Let's go over IQ9\n",
    "    * Work on your project (Cleaning, EDA, and at least 1 hypothesis test)\n",
    "        * Check-in due in class on 5/29 at the latest\n",
    "        * Bonus points for demoing early (this week: 4/22-4/25) during office hours\n",
    "    * Any DA7 questions? Due Sunday night. Today we will cover linear regression\n",
    "    * Note: office hours tomorrow (4/23) cancelled. If you need me, I'll be in my office at 4:15pm or email me\n",
    "* Today\n",
    "    * Cross validation (magnet demo + code in [kNN.ipynb on Github](https://github.com/GonzagaCPSC222/U7-Machine-Learning-NLP/blob/master/ClassificationFun/kNN.ipynb))\n",
    "    * LinearRegression.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "In supervised machine learning, when your \"class\" attribute is continuous, the machine learning task is called regression instead of classification. There are several regression algorithms in Sci-kit Learn that can be used for these tasks, such as:\n",
    "* [Linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "* [kNN regressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "* [Decision tree regressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n",
    "* [Support vector regressor](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "* Etc.\n",
    "\n",
    "While these algorithms can get complex, it is pretty straightforward to implement our own \"simple\" linear regression algorithm. To do this, consider regression tasks where we have one feature variable (an independent variable, x) and one target variable (a dependent variable, y). Given a training set of (x, y) pairs, we can fit a line y = mx + b that can be used for unseen instances (e.g. x values) to predict a y value.\n",
    "\n",
    "## Simple Linear Regression\n",
    "In scatter plots of (x, y) data, it can be helpful to \"fit a line\"\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC222/U7-Machine-Learning-NLP/master/figures/linear_regression_example.png\" width=\"600\"/>\n",
    "\n",
    "* this can be done via linear regression\n",
    "* we're going to look at a simple approach called \"Least Squares\"\n",
    "\n",
    "The basic idea: Given a set of points, find a line that \"best\" fits the points\n",
    "* i.e., find values for $m$ (slope) and $b$ (intercept) that best fits $y = mx + b$\n",
    "\n",
    "In least squares linear regression\n",
    "* find $m$ and $b$ that minimizes the sum of the (vertical) squared distance to the measured data points\n",
    "* once we find $m$, finding $b$ isn't difficult\n",
    "\n",
    "The basic least squares approach:\n",
    "1. Calculate the mean $\\bar{x}$ of the $x$ values and the mean $\\bar{y}$ of the $y$ values\n",
    "    * Note the line must go through the point ($\\bar{x}$, $\\bar{y}$)\n",
    "2. Calculate the slope using the means (where n is the number of data points):\n",
    "$$m = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}$$\n",
    "3. Calculate the y intercept as b = $\\bar{y} - m\\bar{x}$\n",
    "     * or, $\\bar{y} = m\\bar{x} + b$ ... since we know it must go through ($\\bar{x}$, $\\bar{y}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "We can perform linear regression with multiple independent variables (e.g. $x_1, x_2, ..., x_n$) using [Sci-kit Learn's `LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). Like classifiers in Sci-kit Learn, `LinearRegression` has `fit(X_train, y_train)`, `predict(X_test)`, and `score(X_test, y_test)` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Evaluation Metrics\n",
    "The correlation coefficient $r$ helps checks how good the linear relationship is:\n",
    "$$r = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2 \\sum_{i=1}^{n}(y_i - \\bar{y})^2}}$$\n",
    "* Note the bottom is essentially the same as the top just squared to strip away\n",
    "the signs\n",
    "* If the correlation is perfectly linear, then result is 1\n",
    "* If the correlation is perfect inverse linear, then result is -1\n",
    "* If no relationship, the result is 0\n",
    "\n",
    "An alternative formula (where $\\sigma_x$ is the standard deviation of $x$):\n",
    "$$m = r \\frac{\\sigma_y}{\\sigma_x}$$\n",
    "\n",
    "The coefficient of determination $R^2$ is the correlation coefficient squared, $R^2 = r^2$\n",
    "* $R^2$ is the proportion of variation in the dependent (y) variable that is explained by the independent (x) variable\n",
    "* The higher $R^2$, the better the fit line \n",
    "\n",
    "The covariance can also be used to assess correlation\n",
    "$$cov = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{n}$$\n",
    "* covariance can also be used to calculate the correlation coefficient:\n",
    "$$r = \\frac{cov}{\\sigma_x \\sigma_y}$$\n",
    "\n",
    "The standard error (SE) is also used to help check the fit\n",
    "$$SE = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - y^\\prime)^2}{n}}$$\n",
    "* Where $y^\\prime$ is the \"predicted\" value and $y$ is the actual value\n",
    "* $(y_i - y^\\prime)$ is called a \"residual\"\n",
    "* Note standard error is essentially the standard deviation of the differences: $SE = \\frac{\\sigma}{\\sqrt{n}}$\n",
    "* Lower the value the \"better\" the fit\n",
    "\n",
    "Additional regression evaluation metrics that are commonly used to quantify the residuals:\n",
    "* Mean absolute error (MAE)\n",
    "* Mean square error (MSE)\n",
    "* Root mean square error (RMSE)\n",
    "* Normalized RMSE (NRMSE)\n",
    "* Etc. \n",
    "\n",
    "What does it mean if there is a strong (linear) correlation?\n",
    "* One of the attributes is (potentially) redundant because it is implied by the other\n",
    "* One is a good predictor for the other ... good if one is a class label\n",
    "    * i.e., Regression is one way to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation DOES NOT Imply Causation\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-13d22f6fda3811a9108d18b71c46e933\" width=\"400\"/>\n",
    "\n",
    "(image from https://qph.fs.quoracdn.net/main-qimg-13d22f6fda3811a9108d18b71c46e933)\n",
    "\n",
    "It is important to note that a strong correlation between two variables does not mean that one causes the other. Correlation does not account for extraneous variables, such as:\n",
    "* Weather\n",
    "* Seasonality\n",
    "* Competitor\n",
    "* Economy\n",
    "* Pricing\n",
    "* etc.\n",
    "\n",
    "For example consider how ice cream consumption and sunburn prevalence are correlated, but neither \"causes\" the other because there is an extraneous variable, sunny weather:\n",
    "\n",
    "<img src=\"https://pbs.twimg.com/media/ENXkNi2X0AAqLyI?format=jpg&name=medium\" width=\"500\"/>\n",
    "\n",
    "(image from https://pbs.twimg.com/media/ENXkNi2X0AAqLyI?format=jpg&name=medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolating Causation and Experiment Design\n",
    "As another example, consider how changes in weather are correlated with increases in product sales:\n",
    "    \n",
    "<img src=\"https://www.businessprocessincubator.com/wp-content/uploads/2017/10/www.capgemini.comweeklysalesincreaseusatem-0f0687fe4fda13494079d48aa4b6ba5521ecb085.png\" width=\"400\"/>\n",
    "\n",
    "(image from https://www.businessprocessincubator.com/content/understanding-the-sales-landscape-with-an-extremely-randomized-forest/)\n",
    "\n",
    "If a hedge trimmer company rolled out a new marketing campaign at the start of summer, they may attribute their increase in sales to the marketing campaign because sales are correlated with the marketing budget/efforts; however, as the infographic above shows, weather is an extraneous variable that is more likely the real cause of the increased sales during the summer.\n",
    "\n",
    "To better estimate the effects of the campaign, the hedge trimmer company could compare sales from this summer (with a marketing campaign) to a previous summer (without a marketing campaign) and try to control for other extraneous/confounding variables. \n",
    "\n",
    "In general, researchers typically isolate causality with an effectively designed experiment, such as a randomized control trial where there are two groups (control and experiment) that are identical except for one variable (the one hypothesize is the cause of a certain outcome):\n",
    "\n",
    "<img src=\"https://quantifyinghealth.com/wp-content/uploads/2020/01/rct-vs-cohort-study-design.png\" width=\"500\"/>\n",
    "\n",
    "(image from https://quantifyinghealth.com/wp-content/uploads/2020/01/rct-vs-cohort-study-design.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
