{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 222](https://github.com/GonzagaCPSC222) Intro to Data Science\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Classifier Evaluation\n",
    "What are our learning objectives for this lesson?\n",
    "* Evaluate classifier performance using different metrics\n",
    "* Divided a dataset into training and testing sets using different approaches\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s)\n",
    "1. Download the shirt_sizes_long.csv file from Github (it is in the ClassificationFun folder)\n",
    "    * In kNN.ipynb, load this dataset into a dataframe\n",
    "    * We are going to use this slightly larger dataset to explore different ways to divide a dataset into training and testing sets\n",
    "1. Take a look at this tutorial to see an overview of how decision trees work: http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\n",
    "    * Read about [sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "    * We are going to see how this classifier performs compared to kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today\n",
    "* Announcements\n",
    "    * Work on your project (Cleaning, EDA, and at least 1 hypothesis test)\n",
    "        * Check-in due sometime after we come back from Easter break (in class on 5/29 at the latest)\n",
    "        * Bonus points for demoing early (week of 4/22-4/25) during office hours\n",
    "    * Let's go over DA7 and take questions\n",
    "    * Game dev talk tonight 6pm Bollier 120 -- Pizza and snacks!!\n",
    "    * Have a great Easter weekend üê£\n",
    "* Today\n",
    "    * ClassificationFun\n",
    "        * Closing thoughts on kNN\n",
    "        * Evaluating classifier performance\n",
    "        * (if time) Decision tree example\n",
    "    * IQ9 last ~15 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classifier Performance\n",
    "Divide data set into a Training Set and a Test Set\n",
    "* \"Build\" classifier on training set\n",
    "* Test performance on the test set (try to predict their labels)\n",
    "* For the test set you know the \"ground truth\"\n",
    "    * More on how to do this later\n",
    "\n",
    "Assume we have 2-valued class labels (e.g., \"yes\" and \"no\")\n",
    "* As an example, the titanic data set (more later)\n",
    "\n",
    "|status |age |gender |survived|\n",
    "|-|-|-|-|\n",
    "|crew |adult |female |yes|\n",
    "|first |adult |male |no|\n",
    "|crew |child |female |no|\n",
    "|second |adult |male |yes|\n",
    "\n",
    "* We want to predict/classify survival (i.e., survived is the class)\n",
    "    * Positive instances: instances of the \"main\" class of interest (e.g., yes label)\n",
    "    * Negative instances: all the other instances\n",
    "* $P$ = the # of positive instances in our test set\n",
    "* $N$ = the # of negative instances in our test set\n",
    "* $TP$ = (True Positives) = # of positive instances we classified as positive\n",
    "* $TN$ = (True Negatives) = # of negative instances we classified as negative\n",
    "    * Combined, these are our \"successful\" predictions\n",
    "* $FP$ (False Positives) = # of negative instances we classified as positive\n",
    "* $FN$ (False Negatives) = # of positive instances we classified as negative\n",
    "    * Combined, these are our \"failed\" predictions\n",
    "\n",
    "A generalized \"confusion matrix\" for (binary) classification\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC222/U7-Machine-Learning-NLP/master/figures/binary_confusion_matrix.png\" width=\"400\">\n",
    "\n",
    "## Metrics\n",
    "### Accuracy\n",
    "Accuracy: % of test instances correctly classified by the classifier\n",
    "$$Accuracy = \\frac{TP + TN}{P + N} = \\frac{TP + TN}{TP + FP + TN + FN}$$\n",
    "* Sometimes called \"recognition rate\"\n",
    "* Use the [KNeighborsClassifier.score(X, y)](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.score) method to determine the accuracy of a kNN classifier \n",
    "        * Description: \"Return the mean accuracy on the given test data and labels.\"\n",
    "    * Note: for other classifiers, check the documentation for the `score()` method to see what the default evaluation metric is\n",
    "* Warning: can be skewed if unbalanced distribution of class labels\n",
    "    * e.g., lots of negative cases that are easily detected (e.g. 99% accuracy when 99% of the dataset is the negative class)\n",
    "    * shadows performance on positive cases\n",
    "    \n",
    "### Example\n",
    "What is the accuracy for the following binary classification confusion matrix?\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC222/U6-Machine-Learning/master/figures/accuracy_exercise.png\" width=\"300\">\n",
    "\n",
    "[//]: # ($$Accuracy = \\frac{TP + TN}{P + N} = \\frac{18 + 8}{40} \\approx 60\\%$$)\n",
    "\n",
    "### Error Rate\n",
    "Error Rate: 1 - accuracy\n",
    "$$ErrorRate = \\frac{FP + FN}{P + N}$$\n",
    "* Has same issues as accuracy (unbalanced labels)\n",
    "* For multi-class classification, can take the average error rate per class\n",
    "\n",
    "### More Classifier Evaluation Metrics to Look Into\n",
    "* Precision: measure of \"exactness\"\n",
    "* Recall (AKA sensitivity): measure of \"completeness\"\n",
    "* F-Measure (AKA F1 score): combine the two via the harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "Building a classifier starts with a learning (training) phase\n",
    "* Based on predefined set of examples (AKA the training set)\n",
    "\n",
    "The classifier is then evaluated for predictive accuracy\n",
    "* Based on another set of examples (AKA the testing set)\n",
    "* We use the actual labels of the examples to test the predictions\n",
    "\n",
    "In general, we want to try to avoid overfitting\n",
    "* That is, encoding particular characteristics/anomalies of the training set into the classifier\n",
    "* Similar notion is \"underfitting\" (too simple of a model, e.g., linear instead of polynomial)\n",
    "\n",
    "We are going to discuss different ways to select training and testing sets\n",
    "1. The Holdout method\n",
    "2. Random Subsampling\n",
    "3. $k$-Fold Cross Validation and Variants\n",
    "4. Bootstrap Method\n",
    "\n",
    "### Holdout Method\n",
    "In the holdout method, the dataset is divided into two sets, the training and the testing set. The training set is used to build the model and the testing set is used to evaluate the model (e.g. the model's accuracy).\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Supervised_machine_learning_in_a_nutshell.svg/2000px-Supervised_machine_learning_in_a_nutshell.svg.png)\n",
    "(image from https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Supervised_machine_learning_in_a_nutshell.svg/2000px-Supervised_machine_learning_in_a_nutshell.svg.png)\n",
    "\n",
    "Approaches to the holdout method\n",
    "* Randomly divide data set into a training and test set\n",
    "* Partition evenly or, e.g., $\\frac{2}{3}$ to $\\frac{1}{3}$ (2:1) training to test set\n",
    "* This is random selection without replacement\n",
    "* Use the [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to apply the holdout method to a dataset\n",
    "    * Description: \"Split arrays or matrices into random train and test subsets\"\n",
    "    * `test_size` parameter: \"If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If train_size is also None, it will be set to 0.25\"\n",
    "    * `random_state` parameter: \"Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls.\"\n",
    "    * `stratify` parameter: \"If not None, data is split in a stratified fashion, using this as the class labels.\"\n",
    "\n",
    "### Random Subsampling Method\n",
    "* Repeat the holdout method $k$ times\n",
    "* Accuracy estimate is the average of the accuracy of each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Fold Cross-Validation Method\n",
    "One of the shortcomings of the hold out method is the evaluation of the model depends heavily on which examples are selected for training versus testing. K-fold cross validation is a model evaluation approach that addresses this shortcoming of the holdout method.\n",
    "* Initial dataset partitioned into $k$ subsets (\"folds\") $D_1, D_2,..., D_k$\n",
    "* Each fold is approximately the same size\n",
    "* Training and testing is performed $k$ times:\n",
    "    * In iteration $i$, $D_i$ is used as the test set\n",
    "    * And $D_1 \\cup ... \\cup D_{i‚àí1} \\cup D_{i+1} \\cup ... \\cup D_k$ used as training set\n",
    "* Note each subset is used exactly once for testing\n",
    "* Accuracy estimate is number of correct classifications over the $k$ iterations, divided by total number of rows (i.e., test instances) in the initial dataset\n",
    "    * Alternatively, average accuracy by label\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)\n",
    "(image from https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)\n",
    "* Use the [cross_val_score(estimator, X, y)](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) function to apply k-fold cross-validation to a dataset\n",
    "    * Description: \"Split arrays or matrices into random train and test subsets\"\n",
    "    * `cv` parameter: \"Determines the cross-validation splitting strategy.\" By default it is set to 5-fold cross validation, but you can pass in an to specify the number of folds in a (Stratified)KFold\n",
    "    * `scoring` parameter: \"A str (see model evaluation documentation) or a scorer callable object\". By default it is set to the estimator's default scorer (if available)\n",
    "\n",
    "### More Approaches to Building a Test Set to Look Into\n",
    "* Leave-one-out method cross validation\n",
    "    * Special case of cross-validation where $k$ is the number of instances\n",
    "* Stratified Cross-Validation method\n",
    "    * Class distribution within folds is approximately the same as in the initial data\n",
    "* The Bootstrap Method\n",
    "    * Like random subsampling but with replacement\n",
    "    * Usually used for small datasets"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
