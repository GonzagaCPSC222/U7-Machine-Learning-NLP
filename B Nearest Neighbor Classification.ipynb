{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 222](https://github.com/GonzagaCPSC222) Intro to Data Science\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Nearest Neighbor Classification\n",
    "What are our learning objectives for this lesson?\n",
    "* Learn about the kNN classification algorithm\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s)\n",
    "1. Finish your MA12 w/your partner\n",
    "1. Ask me any DA6 questions you might have\n",
    "1. Open VS Code or Jupyter Lab in a new folder called ClassificationFun\n",
    "    * Create a new notebook called kNN.ipynb\n",
    "    * Create a `DataFrame` for the following data:\n",
    "```\n",
    "height(cm),weight(kg),t-shirt size\n",
    "158,58,M\n",
    "163,61,M\n",
    "165,61,L\n",
    "168,66,L\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today\n",
    "* Announcements\n",
    "    * MA13 notecard quiz next class!\n",
    "    * DA6 is due tonight. Come to my office hours or TA Alicia's office hours to check your step 1s\n",
    "    * Work on MA13 and your project this weekend\n",
    "    * Tonight: software copyright & AI talk. PACCAR 107 at 6pm\n",
    "* MA12/DA6 work time\n",
    "    * MA12 solutions\n",
    "* Starting our machine learning unit with the kNN Lab üè†\n",
    "* IQ8 last ~15 mins of class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s) 4/15\n",
    "1. Pull out your MA13 note sheet and the kNN Lab\n",
    "1. Open ClassificationFun/kNN.ipynb\n",
    "1. That's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today 4/15\n",
    "* Announcements\n",
    "    * Nice job on IQ8, let's go over it\n",
    "    * IQ9 next class on U6/DA6 (emphasis on confidence intervals and hypothesis testing)\n",
    "        * As long as you don't have a bunch of notes/writing on your stats cheat sheet, you can use it on the quiz!\n",
    "        * If you need to print a fresh stats cheat sheet, you can find it in our [Google Drive folder](https://drive.google.com/drive/u/0/folders/1mzkIiJwJM_R4Vjf6YIdKqbwVtCg66Ncl)\n",
    "    * DA7 is posted. Please read through it before next class\n",
    "    * Work on your project (Cleaning, EDA, and at least 1 hypothesis test)\n",
    "        * Check-in due sometime after we come back from Easter break (in class on 4/29 at the latest)\n",
    "        * Bonus points for demoing early (week of 4/22-4/25) during office hours\n",
    "    * Reminder about game dev talk Thursday night 6pm Bollier 120\n",
    "* MA13\n",
    "* Finish kNN Lab\n",
    "* ClassificationFun (finish kNN lab + kNN code in kNN.ipynb)\n",
    "* Go over MA12 solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Classification\n",
    "Nearest neighbor classification is typically used when attributes are continuous\n",
    "* Can be modified for categorical data ...\n",
    "\n",
    "Basic approach:\n",
    "* Given an instance $i$ with $n - 1$ attributes (where the $n^{th}$ is class label)\n",
    "* Find the \"closest\" instance $j$ to $i$ on the $n - 1$ attributes\n",
    "* Use $j$'s class as the prediction for $i$\n",
    "\n",
    "Example from [Bramer](https://www.amazon.com/Principles-Mining-Undergraduate-Computer-Science/dp/1447148835):\n",
    "Given the data set\n",
    "\n",
    "|a |b |c |d |e |f |Class|\n",
    "|-|-|-|-|-|-|-|\n",
    "|yes |no |no |6.4 |8.3 |low |negative|\n",
    "|yes |yes |yes |18.2 |4.7 |high |positive|\n",
    "\n",
    "What should this instance's classification be?\n",
    "\n",
    "|yes |no |no |6.6 |8.0 |low |???|\n",
    "|-|-|-|-|-|-|-|\n",
    "\n",
    "Usually it isn't this easy!\n",
    "\n",
    "## k Nearest Neighbors\n",
    "Find the k nearest neighbors ...\n",
    "* Usually find the k closest neighbors (instead of just closest)\n",
    "* Then pick classification from among the top k\n",
    "\n",
    "What are good values for the number of neighbors k?\n",
    "* Often done experimentally (with a test set)\n",
    "* Start with k = 1, determine \"error rate\" (more later)\n",
    "* Repeat incrementing k\n",
    "* Pick k with smallest (minimum) error rate\n",
    "     * Often, larger the data (training) set, the larger the k\n",
    "\n",
    "Lets say we found the k nearest neighbors for an instance ...\n",
    "\n",
    "Q: What are ways we could pick the class?\n",
    "* Most frequent occurring class\n",
    "* Weighted \"average\" (based on the relative closest of the k)\n",
    "\n",
    "Note: can use k-NN for regression if, e.g., return the mean of the label values\n",
    "\n",
    "## Distance Functions\n",
    "k-NN works by calculating distances between instances\n",
    "* Many possible ways to do this ... generalized through \"distance measures\"\n",
    "* For two points $x$ and $y$, the distance between them is given by $dist(x,y)$\n",
    "\n",
    "Properties of distance measures (metrics)\n",
    "1. $\\forall x$, $dist(x,x) = 0$\n",
    "    * The distance of any point x from itself is zero\n",
    "2. $\\forall xy$, $dist(x,y) = dist(y,x)$\n",
    "    * Symmetry\n",
    "3. $\\forall xyz$, $dist(x,y) \\leq dist(x,z) + dist(z,y)$\n",
    "    * Triangle equality\n",
    "    * \"Shortest distance between any two points is a straight line\"\n",
    "\n",
    "Euclidean Distance is most often used: Given an instance, treat it as a \"vector\" in n space\n",
    "* Use Pythagoras' Theorem to find distance between them\n",
    "\n",
    "For example:\n",
    "* Given two points (i.e., rows) with $n = 2$: $(x_1, y_1)$ and $(x_2, y_2)$\n",
    "* The length (i.e., distance) of the straight line joining the points is:\n",
    "\n",
    "$$\\sqrt{(x_1 - x_2)^{2} + (y_1 - y_2)^{2}}$$\n",
    "\n",
    "* Euclidean $n$-space\n",
    "    * For rows A = $(a_1, a_2,..., a_n)$ and $B = (b_1, b_2,..., b_n)$ with $n$ attributes\n",
    "$$\\sqrt{(a_1 - b_1)^{2} + (a_2 - b_2)^{2} +...+ (a_n - b_n)^{2}}$$\n",
    "        * Which is:\n",
    "$$\\sqrt{\\sum_{i=1}^{n}(a_i - b_i)^{2}}$$\n",
    "\n",
    "Other examples are described in the book (e.g., Manhattan \"city block\" distance)\n",
    "\n",
    "Q: Do you see any possible issues with Euclidean distance?\n",
    "* Larger values tend to dominate smaller ones\n",
    "* Can degrade into a few attributes driving the distances\n",
    "    * e.g., [Mileage=18,457, Doors=2, Age=12]\n",
    "    \n",
    "## Normalization\n",
    "One solution is to scale all values between 0 and 1 (\"min-max\" normalization)\n",
    "* Use the formula: `(x - min(xs)) / ((max(xs) - min(xs)) * 1.0)`\n",
    "\n",
    "Q: How can we deal with categorical values?\n",
    "* $dist(v_1, v_2) = 0$ if values $v_1 = v_2$\n",
    "    * Same values\n",
    "* For nominal values, $dist(v_1, v_2) = 1$ if $v_1 \\neq v_2$\n",
    "    * Different values\n",
    "* For ordinal values, assign to 1 or use the \"distance\"\n",
    "\n",
    "Q: What do we do about missing values?\n",
    "* Don't have missing values\n",
    "    * Clean the data first\n",
    "* Be conservative\n",
    "    * Assuming normalized\n",
    "    * If only one value missing:\n",
    "        * Assume the maximum possible distance\n",
    "        * If nominal use the maximum distance (i.e., 1)\n",
    "        * If ordinal use either 1 or furthest distance from known value\n",
    "    * otherwise, if both values missing, use the maximum distance (e.g., 1)\n",
    "\n",
    "Distance-based metrics imply equal weighting of attributes\n",
    "* Sometimes can perform better with attribute weights (i.e., some attributes worth more)\n",
    "* \"Feature reduction\" (not using certain attributes) can also help with redundant or \"noisy\" attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic k-NN Algorithm\n",
    "```\n",
    "Input: list of rows, no of atts (n where nth is label), instance to classify, k\n",
    "def kNN_classifier(training_set, n, instance, k):\n",
    "    row_distances = []\n",
    "    for row in training_set:\n",
    "        d = distance(row, instance, n - 1)\n",
    "        row_distances.append([d, row])\n",
    "    top_k_rows = get_top_k(row_distances, k)\n",
    "    label = select_class_label(top_k_rows)\n",
    "    return label\n",
    "```\n",
    "\n",
    "## kNN Example\n",
    "Example adapted from [this kNN example](https://people.revoledu.com/kardi/tutorial/KNN/KNN_Numerical-example.html)\n",
    "\n",
    "Suppose we have the following dataset that has two attributes (acid durability and strength) and a class attribute (whether a special paper tissue is good or not):\n",
    "\n",
    "|Acid durability (seconds)|Strength (kg/square meter)|Classification|\n",
    "|-|-|-|\n",
    "|7|7|Bad|\n",
    "|7|4|Bad|\n",
    "|3|4|Good|\n",
    "|1|4| Good|\n",
    "\n",
    "Now the factory produces a new paper tissue with acid durability = 3 seconds and strength = 7 kg/square meter. Can we predict what the classification of this new tissue is? Use kNN with $k$ = 3. \n",
    "\n",
    "### Make a Prediction Manually\n",
    "Steps:\n",
    "1. Normalize\n",
    "1. Compute distance of each training instance to the test instance\n",
    "1. Determine the majority classification of the $k$ closest instances... this is your prediction for the test instance\n",
    "\n",
    "After normalization:\n",
    "\n",
    "|Acid durability (seconds)|Strength (kg/square meter)|Classification|\n",
    "|-|-|-|\n",
    "|1|1|Bad|\n",
    "|1|0|Bad|\n",
    "|0.33|0|Good|\n",
    "|0|0| Good|\n",
    "\n",
    "Test instance normalization: 0.33, 1\n",
    "\n",
    "Distances:\n",
    "\n",
    "|Acid durability (seconds)|Strength (kg/square meter)|Classification|Distance|\n",
    "|-|-|-|-|\n",
    "|1|1|Bad|0.66|\n",
    "|1|0|Bad|1.203|\n",
    "|0.33|0|Good|1.0|\n",
    "|0|0| Good|1.05|\n",
    "\n",
    "Work:\n",
    "* $\\sqrt{(1-0.33)^2 + (1-1)^2} = 0.66$\n",
    "* $\\sqrt{(1-0.33)^2 + (0-1)^2} = 1.203$\n",
    "* $\\sqrt{(0.33-0.33)^2 + (0-1)^2} = 1.0$\n",
    "* $\\sqrt{(0-0.33)^2 + (0-1)^2} = 1.05$\n",
    "\n",
    "Majority classification: \n",
    "1 Bad (0.66) and 2 Goods (1.0 an 1.05) => Good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Prediction with Scikit-Learn\n",
    "Steps:\n",
    "1. Load data\n",
    "1. Normalize\n",
    "1. Train kNN classifier with training set\n",
    "1. Test kNN classifier on test instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Acid durability (seconds)  Strength (kg/square meter) Classification\n",
      "0                          7                           7            Bad\n",
      "1                          7                           4            Bad\n",
      "2                          3                           4           Good\n",
      "3                          1                           4           Good\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "\n",
    "data = [[7, 7, \"Bad\"], [7, 4, \"Bad\"], [3, 4, \"Good\"], [1, 4, \"Good\"]]\n",
    "df = pd.DataFrame(data, columns=[\"Acid durability (seconds)\", \"Strength (kg/square meter)\", \"Classification\"])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 4.]\n",
      "[7. 7.]\n",
      "[[1.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.33333333 0.        ]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train = df.drop(\"Classification\", axis=1)\n",
    "y_train = df[\"Classification\"]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "print(scaler.data_min_)\n",
    "print(scaler.data_max_)\n",
    "\n",
    "X_train_normalized = scaler.transform(X_train)\n",
    "print(X_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good']\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train_normalized, y_train)\n",
    "\n",
    "# test\n",
    "X_test = pd.Series([3, 7], index=df.columns.drop(\"Classification\"))\n",
    "X_test = scaler.transform([X_test])\n",
    "y_test_prediction = neigh.predict(X_test)\n",
    "print(y_test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Notes\n",
    "Q: What happens if there are ties in the top-k distances (get_top_k)? E.g., which are top 3 in: [[.28,$r_1$],[.33,$r_2$],[.33,$r_3$],[.33,$r_4$],[.37,$r_5$]]?\n",
    "* Different options ... e.g.:\n",
    "    * Randomly select from ties\n",
    "    * Do top-k distances (instead of instances)\n",
    "    * Ignore ties (in case above, just use $r_1$ and $r_2$)\n",
    "\n",
    "Nearest doesn't imply near\n",
    "* top-k instances might not be that close to the instance being classified\n",
    "* Especially true as the number of attributes (\"dimensions\") increases\n",
    "    * An example of the \"curse of dimensionality\"\n",
    "* Again, have to use common sense and an understanding of the dataset\n",
    "\n",
    "### Efficiency issues\n",
    "Q: Is k-NN efficient? Can you find any efficiency issues?\n",
    "* Given a training set with $D$ instances and $k = 1$\n",
    "* $O(D)$ comparisons needed to classify a given instance\n",
    "\n",
    "Q: Can you think of any ways to improve the efficiency?\n",
    "1. Use search trees\n",
    "    * Presort and arrange instances into a search tree\n",
    "    * Can reduce comparisons to $O(log D)$\n",
    "2. Check each training instance in parallel\n",
    "    * Gives $O(1)$ comparisons\n",
    "3. Editing/Pruning\n",
    "    * Filter or remove training tuples that prove useless\n",
    "    * Reduces size of $D$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
